### 머신러닝

스팸 메일을 판정

그림에서 얼굴을 찾아냄

온라인 쇼핑몰 상품 추천

여태 할 수 없었던 일을 할 수 있게된 사례는 많지만 뭐든지 할 수 있는건 아니다.

응용할 곳은 많지만 만능은 아니다. 어디에 적용할 수 있고, 무엇을 할 수 있고, 할 수 없는지를 이해하는 것도 중요.

### 머신러닝의 중요성

머신러닝의 기초 이론이나 알고리즘 자체는 새로운 것이 아님.

반복처리를 잘하는 컴퓨터를 이용해 많은 데이터를 읽어 들이고 그 데이터 안의 특징을 학습해 패턴을 찾아내는 작업을 인간보다 효율적으로 처리한다. 이런 작업을 머신러닝 혹은 패턴인식이라고 말함.

이런 작업을 컴퓨터에게 맡기려는 생각은 옛날부터 많이 해왔고 연구도 많이 하고 구현해놓은 것도 있음

지금은 할 수 있는 일이 더 많다.

과거와의 차이점

- 데이터를 대량으로 수집할 수 있는 환경이 갖춰짐
  - 인터넷이 발전하면서 개인의 활동이나 생활의 일부가 디지털 쪽으로 옮겨짐 -> 데이터 양이 방대해짐.
  - 웹사이트 접근 로그, 블로그의 글과 사진, 메일 송신 이력, 온라인 쇼핑몰 구매 이력 등등.. -> 데이터 종류가 다양해짐.
- 데이터를 대량으로 처리할 수 있는 환경이 갖춰짐

머신러닝 자체가 데이터로부터 특징이나 패턴을 찾아내는 것이기때문에 데이터가 가장 먼저 필요하다.

얼굴 인식 기능 - SNS에서 인물 태그와 함께 게재된 이미지 데이터 사용

상품 추천 기능 - 온라인 쇼핑몰에 기록된 구매 이력 사용



컴퓨터 성능 향상

- 같은 양의 데이터를 처리하는 시간이 빨라짐.

  - 많이 학습할 수 있음

  - 처리가 빠름

    수치 계산을 GPU에 맡기거나 하둡, 스파크 같은 분산처리 기술도 발달해서 데이터를 대량으로 처리할 수 있는 환경이 만들어짐.

기술이 이제 이론을 따라잡고 있음. 머신러닝에 대한 관심이 커짐. 생활의 편의성을 위한 앱, 사업상에서 인간의 의사결정을 보조. 의료나 금융, 보안 분야 등등 다양한 영역에 응용 가능.



### 머신러닝 알고리즘

머신러닝이 잘할 수 있는 작업

- 회귀(Regression)
- 분류(Classification)
- 클러스터링(Clustering)



**회귀**

시계열 데이터(시간적인 변화를 연속적으로 관측한 데이터. ex. 증권시장의 주가.) 같은 연속된 데이터를 취급할 때 사용하는 기법.

연속된 데이터의 추세를 학습해 이후 추세를 찾아내는 작업 - 회귀를 사용한 머신러닝의 일종.

주가 예측의 경우 과거의 주가에만 영향을 받는 것이 아님. 따라서 과거 주가 데이터만으로는 예측 불가. 무언가를 예측하려면 그것에 영향을 줄 수 있는 데이터를 모아서 조합할 때가 많음.



**분류**

스팸메일 판정 - 메일의 내용과 그 메일이 스팸인지 아닌지를 구분한 데이터를 기반으로 학습. 학습 전에 학습 데이터에 스팸 여부를 표시해야 하기 때문에 귀찮음. 머신러닝에서 가장 힘든 작업은 데이터를 모으는 일. 아직 인간이 개입해야 할 부분은 많다.

분류 대상이 2개 - 2진분류 - ex) 스팸 여부, 사진으로 남녀 구분

분류 대상이 3개이상 - 다항분류 - ex) 숫자 인식 0~9까지로 분류 - 손글씨 숫자가 적힌 이미지와 그 이미지가 실제로 어느 숫자인지를 알려 주는 데이터를 모아 놓은 'MNIST' 데이터셋



**클러스터링(군집화)**

분류와 비슷하지만 다름.  회귀와 분류는 정답 데이터(레이블)를 가지고있음. 클러스터링은 레이블이 붙어 있지 않음. 그룹을 나누는 개념.



회귀, 분류 - 지도학습

클러스터링 - 비지도학습



언어 - 머신러닝에서는 파이썬이나 R 언어 등이 사용되고있음(다른 언어로도 구현할 수는 있지만 파이썬이나 R은 머신러닝을 위한 풍부한 라이브러리가 마련돼 있어서 사용하기 편리함.)



### 회귀

학습 데이터가 n개 있다고 가정하면 학습 데이터마다 생기는 오차의 합은 다음 수식으로 나타낼 수 있다.
$$
E(\theta) = \frac 1 2 \displaystyle \sum_{(i=1)}^n(y^{(i)} - f_\theta(x^{(i)}))^2 \\
x^{(i)} 나\ y^{(i)} 는\ i번째\ 학습\ 데이터를\ 말한다.\\
위\ 식은\ 목적함수라고\ 부르는\ 데\ E(θ)의\ E는\ 오차(Error)를\ 뜻한다.\\
오차에\ 양수와\ 음수가\ 공존할\ 경우\ 합할\ 때\ 상쇄가\ 돼서\ 실제\ 오차가\ 줄어들어서\ 나올\ 것이다.\\
따라서\ (y^{(i)} - f_\theta(x^{(i)}))\ 를\ 제곱해준다.\\
절대값이\ 아닌\ 제곱을\ 사용하는\ 이유는\ 나중에\ 목적함수를\ 미분해야\ 하는데,\\
절대값보다는\ 제곱이\ 미분하기\ 쉽기\ 때문이다.\\
\frac 1 2를\ 곱한\ 이유도\ 나중에\ 미분과\ 관련된\ 식을\ 간단하게\ 만들기\ 위함이다.
$$
각각의 학습 데이터마다 생기는 오차를 제곱해 더한 후 2로 나누었다. 이 E(θ)를 가장 작아지게 하는 θ를 찾는 것이 목적이고, 이를 최적화 문제라고 한다.

